{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Version_Project_IHLT_Augusto_Moran_Jakub_Glowacz.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIgVd-KwDMjf"
      },
      "source": [
        "# IHLT Project: Semantic Textual Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFAI73GQqPuX"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lybwZd6n-0Rw",
        "outputId": "c0e1ee7e-c638-46f4-f7d7-8d014ef9f8e3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "from nltk import ne_chunk, word_tokenize, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "from nltk.metrics import jaccard_distance\n",
        "from nltk.corpus import wordnet_ic\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from scipy.stats import pearsonr\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.util import ngrams\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.collocations import TrigramCollocationFinder\n",
        "nltk.download('wordnet_ic')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('treebank')\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
        "semcor_ic = wordnet_ic.ic('ic-semcor.dat')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGkGo2vo4cHF"
      },
      "source": [
        "import sklearn \n",
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.datasets import make_regression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyKNkVok_GYo",
        "outputId": "e38f8ab5-ee5d-4556-b591-c64b6855b7c5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiQfEVheAjHC"
      },
      "source": [
        "# Functions\n",
        "First we developed functions to calculate the features of sentences learned on the previous labs. In the beginning we created a function which creates a dataframe from sentences that are in the training and testing datasets. Then we wrote functions that read pairs of sentences and perform different featurizing algorithms. The features we used are:\n",
        "- tokenizing\n",
        "- deleting stopwords\n",
        "- lemmatizing\n",
        "- getting part-of-speech tags\n",
        "- name entity chunking\n",
        "- Lesk's algorithm\n",
        "- extracting synsets\n",
        "- extracting n-grams\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "214zqP7fqWRk"
      },
      "source": [
        "Function that creates a dataframe from the STS datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEVV0UZwCXl_"
      },
      "source": [
        "def create_df(files,gs):\n",
        "  files = sorted(files)\n",
        "  gs = sorted(gs)\n",
        "  # we want to concatenate every 'gs' file to be sorted the same as the input files, and we don't know how file with the name 'gs.ALL' is sorted\n",
        "  if 'ALL' in gs[0]:\n",
        "    gs.pop(0)\n",
        "  column0 = []\n",
        "  column1 = []\n",
        "  column2 = []\n",
        "  for filename in files:\n",
        "    with open(filename, encoding=\"utf8\") as txt:\n",
        "      for l in txt:\n",
        "        # appending sentences from txt files to columns\n",
        "        sentences = l.strip().split('\\t')\n",
        "        column0.append(sentences[0])\n",
        "        column1.append(sentences[1])\n",
        "  for filename in gs:\n",
        "    # appending gold standards from txt files to columns\n",
        "    with open(filename, encoding=\"utf8\") as txt:\n",
        "      for l in txt:\n",
        "        gs = l.strip().split('\\t')\n",
        "        column2.append(gs[0])\n",
        "  \n",
        "  # appending columns to dataframe\n",
        "  df = pd.DataFrame()\n",
        "  df['s1'] = column0\n",
        "  df['s2'] = column1\n",
        "  df['gs'] = column2\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcgKqotzrF0j"
      },
      "source": [
        "Function that returns set of words from a sentence, followed by function which creates a list of tokenized sentences from dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKZYXVIZAi0Q"
      },
      "source": [
        "def tokenize(sentence):\n",
        "  sentences = nltk.sent_tokenize(sentence)\n",
        "  # setting the words to lowercase for comparison purposes\n",
        "  tokenized = set(np.concatenate([nltk.word_tokenize(s.lower()) for s in sentences]))\n",
        "  # filtering punctuation\n",
        "  return set([t for t in tokenized if t not in string.punctuation]) \n",
        "\n",
        "def tokenize_df(df):\n",
        "  tokenized1 = []\n",
        "  tokenized2 = []\n",
        "  for sent1, sent2 in zip(df['s1'],df['s2']):\n",
        "    tokenized1.append(tokenize(sent1))\n",
        "    tokenized2.append(tokenize(sent2))\n",
        "  tokenized = [tokenized1,tokenized2]\n",
        "  return tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpEhoHRYrnMa"
      },
      "source": [
        "Function that tokenizes sentences and filters the stopwords, followed by function which creates a list of tokenized sentences from dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_esxQVtElTwH"
      },
      "source": [
        "def tokenize_wout_stopwords(sentence):\n",
        "  sentences = nltk.sent_tokenize(sentence)\n",
        "  # setting the words to lowercase for comparison purposes\n",
        "  tokenized = set(np.concatenate([nltk.word_tokenize(s.lower()) for s in sentences]))\n",
        "  # filtering punctuation and stopwords\n",
        "  return set([t for t in tokenized if t not in stopwords.words('english') and t not in string.punctuation])\n",
        "  \n",
        "def tokenize_wout_stopwords_df(df):\n",
        "  tokenized1 = []\n",
        "  tokenized2 = []\n",
        "  for sent1, sent2 in zip(df['s1'],df['s2']):\n",
        "    tokenized1.append(tokenize_wout_stopwords(sent1))\n",
        "    tokenized2.append(tokenize_wout_stopwords(sent2))\n",
        "  tokenized = [tokenized1,tokenized2]\n",
        "  return tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF9KQvFvsDvI"
      },
      "source": [
        "Function that lemmatizes sentences, followed by function which creates a list of lemmatized sentences from dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HNkh_W2a0D_"
      },
      "source": [
        "def lemmatize(pos_tags):\n",
        "    wnl = WordNetLemmatizer()\n",
        "    # setting pos_tags to lowercase to be able to process it by the WordNetLemmatizer() lemmatizing function\n",
        "    if pos_tags[1][0] in {'N','V', 'A', 'S', 'R'}:\n",
        "        return wnl.lemmatize(pos_tags[0].lower(), pos=pos_tags[1][0].lower())\n",
        "    return pos_tags[0]\n",
        "\n",
        "def lemmatize_df(df):\n",
        "  lemmatized1 = []\n",
        "  lemmatized2 = []\n",
        "  for sent1, sent2 in zip(df['s1'],df['s2']):\n",
        "    lemmatized1.append(set([lemmatize(pair) for pair in nltk.pos_tag(tokenize(sent1))]))\n",
        "    lemmatized2.append(set([lemmatize(pair) for pair in nltk.pos_tag(tokenize(sent2))]))\n",
        "  lemmatized = [lemmatized1, lemmatized2]\n",
        "  return lemmatized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHNjEnsSsO3G"
      },
      "source": [
        "Function which creates a list of lemmatized sentences without stopwords from dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4o2HRbhsY3i"
      },
      "source": [
        "def lemmatize_wout_stopwords_df(df):\n",
        "  lemmatized1 = []\n",
        "  lemmatized2 = []\n",
        "  for sent1, sent2 in zip(df['s1'],df['s2']):\n",
        "    lemmatized1.append(set([lemmatize(pair) for pair in nltk.pos_tag(tokenize_wout_stopwords(sent1))]))\n",
        "    lemmatized2.append(set([lemmatize(pair) for pair in nltk.pos_tag(tokenize_wout_stopwords(sent2))]))\n",
        "  lemmatized = [lemmatized1, lemmatized2]\n",
        "  return lemmatized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrLAYGf1sikE"
      },
      "source": [
        "Function that gets pos tags from words which have them from sentences, followed by function which creates a list of pos-tagged sentences from dataframe. This function only returns the part of speech, without its properties. For example (n - noun, v - verb, instead of NNP, PRP etc.):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbtZf1JQoIVa"
      },
      "source": [
        "def get_pos_tag_simple(sentence):\n",
        "  sentence = tokenize(sentence) \n",
        "  pairs = pos_tag(sentence)\n",
        "  pairs_useful = []\n",
        "  # check witch words have synsets in the wordnet library\n",
        "  for pair in pairs: \n",
        "    x, y = pair[0].lower(), pair[1][0].lower()\n",
        "    try:\n",
        "      wn.synsets(x, y)\n",
        "    except:\n",
        "      pass\n",
        "    else:\n",
        "      pairs_useful.append((x, y[0]))\n",
        "  return pairs_useful\n",
        "  \n",
        "def pos_tag_simple_df(df):\n",
        "  pos_tagged1 = []\n",
        "  pos_tagged2 = []\n",
        "  for sent1, sent2 in zip(df['s1'],df['s2']):\n",
        "    pos_tagged1.append(set(get_pos_tag_simple(sent1)))\n",
        "    pos_tagged2.append(set(get_pos_tag_simple(sent2)))\n",
        "  pos_tagged = [pos_tagged1, pos_tagged2]\n",
        "  return pos_tagged"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function which creates a list of pos-tagged sentences from dataframe:"
      ],
      "metadata": {
        "id": "Aikwd4G-u6nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tag_df(df):\n",
        "  pos_tagged1 = []\n",
        "  pos_tagged2 = []\n",
        "  for sent1, sent2 in zip(df['s1'],df['s2']):\n",
        "    pos_tagged1.append(set(pos_tag(tokenize(sent1))))\n",
        "    pos_tagged2.append(set(pos_tag(tokenize(sent2))))\n",
        "  pos_tagged = [pos_tagged1, pos_tagged2]\n",
        "  return pos_tagged"
      ],
      "metadata": {
        "id": "PXw_704Au_71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMNqOO2Cs_lq"
      },
      "source": [
        "Function that returns list of Name Entity chunks from a dataframe of sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPAbz9PiBa1A"
      },
      "source": [
        "def ne_chunk_df(df):\n",
        "  chunked1_sentence = []\n",
        "  chunked2_sentence = []\n",
        "  chunked1 = []\n",
        "  chunked2 = []\n",
        "  for sent1, sent2 in zip(df['s1'],df['s2']):\n",
        "    ch1 = ne_chunk(pos_tag(word_tokenize(sent1)))\n",
        "    ch2 = ne_chunk(pos_tag(word_tokenize(sent2)))\n",
        "    # iterate through tree and find the Name Entities and append them to lists\n",
        "    for c1 in ch1:\n",
        "      if hasattr(c1, 'label'):\n",
        "        chunked1_sentence.append((c1.label(), ' '.join(c[0] for c in c1)))\n",
        "    chunked1.append(set(chunked1_sentence))\n",
        "    for c2 in ch2:\n",
        "      if hasattr(c2, 'label'):\n",
        "        chunked2_sentence.append((c2.label(), ' '.join(c[0] for c in c2)))\n",
        "    chunked2.append(set(chunked2_sentence))\n",
        "  ne_chunked = [chunked1, chunked2]\n",
        "  return ne_chunked\n",
        "\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmYkEkGhtMnL"
      },
      "source": [
        "Function to apply Lesk’s algorithm to the words in the sentences, followed by function which creates a list of products of Lesk's algorithm applied to sentences from the dataframe. To perform Lesk's algorithm we must obtain the simple pos tags from the sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYVn_fSooSko"
      },
      "source": [
        "def get_lesk(sentence,pairs_useful):\n",
        "  synsets = set()\n",
        "  #finding words which can be processed by Lesk, and appending them\n",
        "  for pair in pairs_useful:\n",
        "    synset = nltk.wsd.lesk(sentence, pair[0], pair[1])\n",
        "    if synset is not None:\n",
        "      synsets.add(synset.name())\n",
        "  return synsets\n",
        "  \n",
        "def lesk_df(df):\n",
        "  lesk0 = []\n",
        "  lesk1 = []\n",
        "  for sentence in df['s1']: # applying lesk algorith to every word in every sentence from column 0\n",
        "    a = get_pos_tag_simple(sentence)\n",
        "    b = get_lesk(sentence, a)\n",
        "    lesk0.append(b)\n",
        "  for sentence in df['s2']: # applying lesk algorith to every word in every sentence from column 1\n",
        "    a = get_pos_tag_simple(sentence)\n",
        "    b = get_lesk(sentence, a)\n",
        "    lesk1.append(b)\n",
        "  lesk = [lesk0, lesk1]\n",
        "  return lesk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEvBuY-Gss8D"
      },
      "source": [
        "Function which creates a list of lenghts differences between sentences from the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3Wq7V90YbRT"
      },
      "source": [
        "def length_difference_df(df):\n",
        "  diff = []\n",
        "  for sent1, sent2 in zip(df['s1'],df['s2']):\n",
        "    # find lenght difference by subsuming the length of sentences and dividing by the lenght of the longer sentence\n",
        "    d = 1 - abs((len(tokenize(sent1)) - len(tokenize(sent2))) / max(len(tokenize(sent1)), len(tokenize(sent1))))\n",
        "    diff.append(d)\n",
        "  return diff\n",
        "\n",
        "def length_difference_wout_stopwords_df(df):\n",
        "  diff = []\n",
        "  for sent1, sent2 in zip(df['s1'],df['s2']):\n",
        "    d = 1 - abs((len(tokenize_wout_stopwords(sent1)) - len(tokenize_wout_stopwords(sent2))) / max(len(tokenize_wout_stopwords(sent1)), len(tokenize_wout_stopwords(sent1))))\n",
        "    diff.append(d)\n",
        "  return diff\n",
        "\n",
        "def length_difference_lemmatized_df(df):\n",
        "  ldl = []\n",
        "  feature = lemmatize_wout_stopwords_df(df)\n",
        "  for i in range(len(feature[0])):\n",
        "    d = 1 - abs( ( len(feature[0][i]) - len(feature[1][i]) ) / max(len(feature[0][i]), len(feature[1][i])))\n",
        "    ldl.append(d)\n",
        "  return ldl\n",
        "\n",
        "def length_difference_lemmatized_wout_stopwords_df(df):\n",
        "  ldlws = []\n",
        "  feature = lemmatize_df(df)\n",
        "  for i in range(len(feature[0])):\n",
        "    d = 1 - abs( ( len(feature[0][i]) - len(feature[1][i]) ) / max(len(feature[0][i]), len(feature[1][i])))\n",
        "    ldlws.append(d)\n",
        "  return ldlws"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXVG7bLupu8w"
      },
      "source": [
        "Fucntion that calculates the ngrams of sentences, followed by function which creates a list of calculated ngrams of all pairs of sentences in the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVFHCocBkHEt"
      },
      "source": [
        "def ngram(sentence, n):\n",
        "  tok = tokenize(sentence)\n",
        "  ng = ngrams(tok,n)\n",
        "  return set(ng)\n",
        "\n",
        "def ngram_df(df, n):\n",
        "  ngram1 = []\n",
        "  ngram2 = []\n",
        "  \n",
        "  for sent1, sent2 in zip(df['s1'],df['s2']):\n",
        "    try:\n",
        "      ngram1.append(ngram(sent1, n))\n",
        "      ngram2.append(ngram(sent2, n))\n",
        "    except:\n",
        "      ngram1.append(set())\n",
        "      ngram2.append(set())\n",
        "  ngrammed = [ngram1, ngram2]\n",
        "  return ngrammed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrOZDoqZjA9"
      },
      "source": [
        "The next feature to retrieve will be the synsets of each words, in fact the most frequent synset given the POS tag of the word in order to calculate the similarity between each word. \n",
        "\n",
        "But first we need to get from each sentence each word wihout punctuations and stopwords. For this we us the useful_words function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZQW9UXBUySi"
      },
      "source": [
        "def useful_words(sentence):\n",
        "    return [word.lower() for word in nltk.word_tokenize(sentence) if word not in string.punctuation and word.lower() not in stopwords.words('english')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If4adlMBaFyZ"
      },
      "source": [
        "Then We obtain the proper pos_tag that works with wordnet library. to do this we modify the format of the postag using the get_useful_pos_Tag mehtod."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiEvdDdG3Dlr"
      },
      "source": [
        "def get_useful_pos_tag(pos_tag):\n",
        "  if pos_tag.startswith('N'):\n",
        "    return 'n'\n",
        "  elif pos_tag.startswith('V'):\n",
        "    return 'v'\n",
        "  elif pos_tag.startswith('J'):\n",
        "    return 'a'\n",
        "  elif pos_tag.startswith('R'):\n",
        "    return 'r'\n",
        "  else:\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd8dZ6b63IBL"
      },
      "source": [
        "import collections\n",
        "# sysnet data structure that holds the value of the synset and the associated pos tag.\n",
        "Synset = collections.namedtuple('Synset',['syns', 'postag'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUk2UGUHahYw"
      },
      "source": [
        "After that we get the most frequent synset given the pos_tag of the word. using the function bellow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRRLMTCY3LMm"
      },
      "source": [
        "def get_synset(pos_tagged_words):\n",
        "  synsets = []\n",
        "  for word, pos_tagg in pos_tagged_words:\n",
        "    useful_pos_tag = get_useful_pos_tag(pos_tagg)\n",
        "    if useful_pos_tag:\n",
        "      try:\n",
        "        # getting the most frequent synset\n",
        "        s =  wn.synsets(word, useful_pos_tag)[0]\n",
        "      except:\n",
        "        s = None\n",
        "      if s:\n",
        "        synset_pair = Synset(s, useful_pos_tag) \n",
        "        synsets.append(synset_pair)   \n",
        "  return synsets  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb-WbpdwasiV"
      },
      "source": [
        "Now we can calculate the similarity of between 2 synsets. For this we will use the following methods: path, wup, lch, lin. Taking into account the condition of each one of them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy5LgUeh3aiI"
      },
      "source": [
        "def synset_path_similarity(ss1, ss2):\n",
        "  score, count = 0.0, 0\n",
        "  for s1 in ss1:\n",
        "    sim_values = []\n",
        "    for s2 in ss2:\n",
        "      #compute the path synset\n",
        "      synset_sim = s1.syns.path_similarity(s2.syns)\n",
        "      if synset_sim:\n",
        "        # only if the synset exists, append it to the list\n",
        "        sim_values.append(synset_sim)\n",
        "    if len(sim_values):\n",
        "      highest = max(sim_values)\n",
        "    else:\n",
        "      highest = 0\n",
        "\n",
        "    score += highest\n",
        "    count += 1\n",
        "  # Average similarity of the sentences \n",
        "  if count:\n",
        "    score /= count\n",
        "    return score\n",
        "  else: \n",
        "    return 0\n",
        "\n",
        "def synset_wup_similarity(ss1, ss2):\n",
        "  score, count = 0.0, 0\n",
        "  for s1 in ss1:\n",
        "    sim_values = []\n",
        "    for s2 in ss2:\n",
        "      synset_sim = s1.syns.wup_similarity(s2.syns)\n",
        "      if synset_sim:\n",
        "        sim_values.append(synset_sim)\n",
        "    if len(sim_values):\n",
        "      highest = max(sim_values)\n",
        "    else:\n",
        "      highest = 0\n",
        "\n",
        "    score += highest\n",
        "    count += 1\n",
        "  # Average similarity of the sentences \n",
        "  if count:\n",
        "    score /= count\n",
        "    return score\n",
        "  else: \n",
        "    return 0\n",
        "\n",
        "def synset_lch_similarity(ss1, ss2):\n",
        "  score, count = 0.0, 0\n",
        "  for s1 in ss1:\n",
        "    sim_values = []\n",
        "    for s2 in ss2:\n",
        "      # condition that the synset must hold: same pos tag\n",
        "      if s1.postag == s2.postag and s1.postag != 'a':\n",
        "        synset_sim = s1.syns.lch_similarity(s2.syns)\n",
        "        if synset_sim:\n",
        "          sim_values.append(synset_sim)\n",
        "    if len(sim_values):\n",
        "      highest = max(sim_values)\n",
        "    else:\n",
        "      highest = 0\n",
        "\n",
        "    score += highest\n",
        "    count += 1\n",
        "  # Average similarity of the sentences \n",
        "  if count:\n",
        "    score /= count\n",
        "    return score\n",
        "  else: \n",
        "    return 0\n",
        "\n",
        "def synset_lin_similarity(ss1, ss2):\n",
        "  score, count = 0.0, 0\n",
        "  for s1 in ss1:\n",
        "    sim_values = []\n",
        "    for s2 in ss2:\n",
        "      # pos tag between synset must be the same and not be an adverb nor adjective.\n",
        "      if s1.postag == s2.postag and s1.postag not in {'a', 'r'}:\n",
        "        try:\n",
        "          synset_sim = s1.syns.lin_similarity(s2.syns,semcor_ic)\n",
        "        except:\n",
        "          synset_sim = 0\n",
        "        if synset_sim:\n",
        "          sim_values.append(synset_sim)\n",
        "    if len(sim_values):\n",
        "      highest = max(sim_values)\n",
        "    else:\n",
        "      highest = 0\n",
        "\n",
        "    score += highest\n",
        "    count += 1\n",
        "  # Average similarity of the sentences \n",
        "  if count:\n",
        "    score /= count\n",
        "    return score\n",
        "  else: \n",
        "    return 0\n",
        "\n",
        "def synset_res_similarity(ss1, ss2):\n",
        "  score, count = 0.0, 0\n",
        "  for s1 in ss1:\n",
        "    sim_values = []\n",
        "    for s2 in ss2:\n",
        "      if s1.postag == s2.postag and s1.postag not in {'a', 'r'}:\n",
        "        synset_sim = s1.syns.res_similarity(s2.syns,semcor_ic)\n",
        "        if synset_sim:\n",
        "          sim_values.append(synset_sim)\n",
        "    if len(sim_values):\n",
        "      highest = max(sim_values)\n",
        "    else:\n",
        "      highest = 0\n",
        "\n",
        "    score += highest\n",
        "    count += 1\n",
        "  # Average similarity of the sentences \n",
        "  if count:\n",
        "    score /= count\n",
        "    return score\n",
        "  else: \n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvCLF2IrcChP"
      },
      "source": [
        "Next we develop a method that integrates the previous function and outputs a list, where each item is a list of similiraties. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy8gvk5yC_zi"
      },
      "source": [
        "# The next code add 4 wordnet similarities column to the dataframe\n",
        "\n",
        "def synsets_similarities(df):\n",
        "  path_similarity = []\n",
        "  wup_similarity = []\n",
        "  lch_similarity = []\n",
        "  lin_similarity = []\n",
        "  res_similarity = []\n",
        "\n",
        "  for i in range(len(df['s1'])):\n",
        "    s0 = df['s1'][i]\n",
        "    s1 = df['s2'][i]\n",
        "   \n",
        "    w0 = useful_words(s0)\n",
        "    w1 = useful_words(s1)\n",
        "   \n",
        "    pos_tag0 = pos_tag(w0)\n",
        "    pos_tag1 = pos_tag(w1)\n",
        "    \n",
        "    ss0 = get_synset(pos_tag0)\n",
        "    ss1 = get_synset(pos_tag1)\n",
        "  \n",
        "    path_score = synset_path_similarity(ss0, ss1)\n",
        "    wup_score = synset_wup_similarity(ss0, ss1)\n",
        "    lch_score = synset_lch_similarity(ss0, ss1)\n",
        "    lin_score = synset_lin_similarity(ss0, ss1)\n",
        "    res_score = synset_res_similarity(ss0, ss1)\n",
        "\n",
        "    path_similarity.append(path_score)\n",
        "    wup_similarity.append(wup_score)\n",
        "    lch_similarity.append(lch_score)\n",
        "    lin_similarity.append(lin_score)\n",
        "    res_similarity.append(res_score)\n",
        "\n",
        "  return path_similarity, wup_similarity, lch_similarity, lin_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET2dpJc0CVkg"
      },
      "source": [
        "Function responsible for calculating frequencies of bigrams in given sentences. Further down, there are functions responsible for finding bigrams frequencies in:\n",
        "- tokenized sentences\n",
        "- lemmatized sentences\n",
        "- tokenized and lemmatized sentences without stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPxH3bkizN1t"
      },
      "source": [
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.collocations import TrigramCollocationFinder\n",
        "\n",
        "def bigram_frequency(sent1, sent2):\n",
        "  bigrams1 = []\n",
        "  bigrams2 = []\n",
        "  frequencies1 = []\n",
        "  frequencies2 = []\n",
        "  # find the bigrams of the sentence\n",
        "  collocations1 = BigramCollocationFinder.from_words(sent1)\n",
        "  collocations2 = BigramCollocationFinder.from_words(sent2)\n",
        "  for c1 in collocations1.ngram_fd.items():\n",
        "    bigrams1.append(c1[0])\n",
        "    frequencies1.append(c1[1])\n",
        "  for c2 in collocations2.ngram_fd.items():\n",
        "    bigrams2.append(c2[0])\n",
        "    frequencies2.append(c2[1])\n",
        "  freq = []\n",
        "  # calculate the frequency of occurences of the bigrams\n",
        "  for i, b1 in enumerate(bigrams1):\n",
        "    if b1 in bigrams2:\n",
        "      freq.append(min(frequencies1[i], frequencies2[bigrams2.index(b1)]))\n",
        "  avg_len = (len(sent1) + len(sent2))/2\n",
        "  frequency = sum(freq)/avg_len\n",
        "  return frequency\n",
        "  \n",
        "def bigram_frequency_tokenized_df(df):\n",
        "  bigrammed = []\n",
        "  for sent1, sent2 in zip(df['s1'],df['s2']):\n",
        "    bigrammed.append(bigram_frequency(tokenize(sent1), tokenize(sent2)))\n",
        "  return bigrammed\n",
        "\n",
        "def bigram_frequency_tokenized_wout_stopwords_df(df):\n",
        "  bigrammed = []\n",
        "  for sent1, sent2 in zip(df['s1'],df['s2']):\n",
        "    bigrammed.append(bigram_frequency(tokenize_wout_stopwords(sent1), tokenize_wout_stopwords(sent2)))\n",
        "  return bigrammed\n",
        "\n",
        "def bigram_frequency_lemmatized_df(df):\n",
        "  bfl = []\n",
        "  feature = lemmatize_df(df)\n",
        "  for i in range(len(feature[0])):\n",
        "    bfl.append(bigram_frequency(feature[0][i],feature[1][i]))\n",
        "  return bfl\n",
        "\n",
        "def bigram_frequency_lemmatized_wout_stopwords_df(df):\n",
        "  bfl = []\n",
        "  feature = lemmatize_wout_stopwords_df(df)\n",
        "  for i in range(len(feature[0])):\n",
        "    bfl.append(bigram_frequency(feature[0][i],feature[1][i]))\n",
        "  return bfl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce308DqEC3AK"
      },
      "source": [
        "Function responsible for calculating frequencies of trigrams in given sentences. Further down, there are functions responsible for finding trigrams frequencies in:\n",
        "- tokenized sentences\n",
        "- lemmatized sentences\n",
        "- tokenized and lemmatized sentences without stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVGxtkpxBBRp"
      },
      "source": [
        "def trigram_frequency(sent1, sent2):\n",
        "  trigrams1 = []\n",
        "  trigrams2 = []\n",
        "  frequencies1 = []\n",
        "  frequencies2 = []\n",
        "  collocations1 = TrigramCollocationFinder.from_words(sent1)\n",
        "  collocations2 = TrigramCollocationFinder.from_words(sent2)\n",
        "  for c1 in collocations1.ngram_fd.items():\n",
        "    trigrams1.append(c1[0])\n",
        "    frequencies1.append(c1[1])\n",
        "  for c2 in collocations2.ngram_fd.items():\n",
        "    trigrams2.append(c2[0])\n",
        "    frequencies2.append(c2[1])\n",
        "  freq = []\n",
        "  for i, b1 in enumerate(trigrams1):\n",
        "    if b1 in trigrams2:\n",
        "      freq.append(min(frequencies1[i], frequencies2[trigrams2.index(b1)]))\n",
        "  avg_len = (len(sent1) + len(sent2))/2\n",
        "  frequency = sum(freq)/avg_len\n",
        "  return frequency\n",
        "  \n",
        "def trigram_frequency_tokenized_df(df):\n",
        "  trigrammed = []\n",
        "  for sent1, sent2 in zip(df['s1'],df['s2']):\n",
        "    trigrammed.append(trigram_frequency(tokenize(sent1), tokenize(sent2)))\n",
        "  return trigrammed\n",
        "\n",
        "def trigram_frequency_tokenized_wout_stopwords_df(df):\n",
        "  trigrammed = []\n",
        "  for sent1, sent2 in zip(df['s1'],df['s2']):\n",
        "    trigrammed.append(trigram_frequency(tokenize_wout_stopwords(sent1), tokenize_wout_stopwords(sent2)))\n",
        "  return trigrammed\n",
        "\n",
        "def trigram_frequency_lemmatized_df(df):\n",
        "  tfl = []\n",
        "  feature = lemmatize_df(df)\n",
        "  for i in range(len(feature[0])):\n",
        "    tfl.append(trigram_frequency(feature[0][i],feature[1][i]))\n",
        "  return tfl\n",
        "\n",
        "def trigram_frequency_lemmatized_wout_stopwords_df(df):\n",
        "  tfl = []\n",
        "  feature = lemmatize_wout_stopwords_df(df)\n",
        "  for i in range(len(feature[0])):\n",
        "    tfl.append(trigram_frequency(feature[0][i],feature[1][i]))\n",
        "  return tfl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFvtEexTtxkm"
      },
      "source": [
        "Function that calculates jaccard similarity, followed by function which creates a list of jaccard similarities between features retrieved from the sentences from a dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87dFIhVow74g"
      },
      "source": [
        "def jaccard_similarity(x, y):\n",
        "  return 1 -  jaccard_distance(x, y)\n",
        "\n",
        "def jaccard_similarity_list(feature):\n",
        "  jac_sim_list = []\n",
        "  for i in range(len(feature[0])):\n",
        "    try:\n",
        "      jac_sim_list.append(jaccard_similarity(feature[0][i],feature[1][i]))\n",
        "    except:\n",
        "      jac_sim_list.append(0)\n",
        "  return jac_sim_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_lopeD8pS9_"
      },
      "source": [
        "Function responsible for calculating cosine similarity, followed by function which creates a list of cosine similarities between features retrieved from the sentences from a dataframe. It will be necesary for computing ngrams, because it takes into account duplicates of elements, which jaccard similarity does not consider:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsBwvIqSm1xb"
      },
      "source": [
        "def cosine_similarity(x, y):\n",
        "  o1 =[]\n",
        "  o2 =[]\n",
        "  rvec = x.union(y) \n",
        "  for item in rvec:\n",
        "    # if item appear in both sentences, append 1, else append 0\n",
        "      if item in x: \n",
        "        o1.append(1) \n",
        "      else: \n",
        "        o1.append(0)\n",
        "      if item in y: \n",
        "        o2.append(1)\n",
        "      else: \n",
        "        o2.append(0)\n",
        "  c = 0\n",
        "  # formula for cosine similairty \n",
        "  for i in range(len(rvec)):\n",
        "          c+= o1[i]*o2[i]\n",
        "  cos_sim = c / float((sum(o1)*sum(o2))**0.5)\n",
        "  return cos_sim\n",
        "\n",
        "def cosine_similarity_list(feature):\n",
        "  cos_sim_list = []\n",
        "  for i in range(len(feature[0])):\n",
        "    try:\n",
        "      cos_sim_list.append(jaccard_similarity(feature[0][i],feature[1][i]))\n",
        "    except:\n",
        "      cos_sim_list.append(0)\n",
        "  return cos_sim_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6mBZemiHRa6"
      },
      "source": [
        "Function that combines all the features and calculate jaccard distances between featurized sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYRVhpLE29nP"
      },
      "source": [
        "def get_jaccard_features(df):\n",
        "  features = []\n",
        "  tokenized_list = jaccard_similarity_list(tokenize_df(df))\n",
        "  tokenized_wout_stopwords_list = jaccard_similarity_list(tokenize_wout_stopwords_df(df))\n",
        "  lemmatized_list = jaccard_similarity_list(lemmatize_df(df))\n",
        "  lemmatized_wout_stopwords_list = jaccard_similarity_list(lemmatize_wout_stopwords_df (df))\n",
        "  lesk_list = jaccard_similarity_list(lesk_df(df))\n",
        "  pos_simple_list = jaccard_similarity_list(pos_tag_df(df))\n",
        "  pos_list = jaccard_similarity_list(pos_tag_df(df))\n",
        "  ne_chunk_list = jaccard_similarity_list(ne_chunk_df(df))\n",
        "  length_difference_list = length_difference_df(df)\n",
        "  length_difference_wout_stopwords_list= length_difference_wout_stopwords_df(df)\n",
        "  length_difference_lemmatized_list= length_difference_lemmatized_df(df)\n",
        "  length_difference_lemmatized_wout_stopwords_list= length_difference_lemmatized_wout_stopwords_df(df)\n",
        "  ngram2_list = ngram_df(df,2)\n",
        "  ngram3_list = ngram_df(df,3)\n",
        "  ngram4_list = ngram_df(df,4)\n",
        "  bigrams_frequency_list = bigram_frequency_tokenized_df(df)\n",
        "  bigrams_frequency_wout_stopwords_list = bigram_frequency_tokenized_wout_stopwords_df(df)\n",
        "  bigrams_frequency_lemmatized_list = bigram_frequency_lemmatized_df(df)\n",
        "  bigrams_frequency_lemmatized_wout_stopwords_list = bigram_frequency_lemmatized_wout_stopwords_df(df)\n",
        "  trigrams_frequency_list = bigram_frequency_tokenized_df(df)\n",
        "  trigrams_frequency_wout_stopwords_list = trigram_frequency_tokenized_wout_stopwords_df(df)\n",
        "  trigrams_frequency_lemmatized_list = trigram_frequency_lemmatized_df(df)\n",
        "  trigrams_frequency_lemmatized_wout_stopwords_list = trigram_frequency_lemmatized_wout_stopwords_df(df)\n",
        "\n",
        "\n",
        "  path_similarity, wup_similarity, lch_similarity, lin_similarity = synsets_similarities(df)\n",
        "\n",
        "  for i, f in enumerate(tokenized_list):\n",
        "    features.append([\n",
        "                    tokenized_list[i], \n",
        "                    tokenized_wout_stopwords_list[i], \n",
        "                    lemmatized_list[i], \n",
        "                    lemmatized_wout_stopwords_list[i], \n",
        "                    lesk_list[i], \n",
        "                    # pos_list[i], \n",
        "                    pos_simple_list[i],\n",
        "                    # ne_chunk_list[i], \n",
        "                    path_similarity[i], \n",
        "                    wup_similarity[i], \n",
        "                    lch_similarity[i], \n",
        "                    lin_similarity[i],\n",
        "                    length_difference_list[i],\n",
        "                    length_difference_wout_stopwords_list[i], \n",
        "                    length_difference_lemmatized_list[i],\n",
        "                    length_difference_lemmatized_wout_stopwords_list[i],\n",
        "                    # cosine_similarity_list(ngram2_list)[i],\n",
        "                    # cosine_similarity_list(ngram3_list)[i],\n",
        "                    # cosine_similarity_list(ngram4_list)[i],\n",
        "                    bigrams_frequency_list[i],\n",
        "                    bigrams_frequency_wout_stopwords_list[i],\n",
        "                    bigrams_frequency_lemmatized_list[i],\n",
        "                    bigrams_frequency_lemmatized_wout_stopwords_list[i],\n",
        "                    trigrams_frequency_list[i],\n",
        "                    trigrams_frequency_wout_stopwords_list[i],\n",
        "                    trigrams_frequency_lemmatized_list[i],\n",
        "                    trigrams_frequency_lemmatized_wout_stopwords_list[i]\n",
        "                       ])\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdTiuv__B6JH"
      },
      "source": [
        "# TRAINING PART\n",
        "For the training, we used the train folder from STS dataset. We computed jaccard similarities of every feature as well as the synset similarities. Then we provided the features and the golden standard, to train the Support Vector Regression(SVR), as well as the linear regression model. Then we chose the one which got better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9wme54R4Rwo"
      },
      "source": [
        "Importing the train dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "TeKQZfxzVsip",
        "outputId": "f5dc89fa-a2ca-4fbb-ecfc-6affeed088c0"
      },
      "source": [
        "files_train = glob.glob('/content/drive/MyDrive/IHLT/data_STS/train/STS.input.*')\n",
        "gs_train = glob.glob('/content/drive/MyDrive/IHLT/data_STS/train/STS.gs.*')\n",
        "df_train = create_df(files_train,gs_train)\n",
        "df_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>gs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>But other sources close to the sale said Viven...</td>\n",
              "      <td>But other sources close to the sale said Viven...</td>\n",
              "      <td>4.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Micron has declared its first quarterly profit...</td>\n",
              "      <td>Micron's numbers also marked the first quarter...</td>\n",
              "      <td>3.750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The fines are part of failed Republican effort...</td>\n",
              "      <td>Perry said he backs the Senate's efforts, incl...</td>\n",
              "      <td>2.800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The American Anglican Council, which represent...</td>\n",
              "      <td>The American Anglican Council, which represent...</td>\n",
              "      <td>3.400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The tech-loaded Nasdaq composite rose 20.96 po...</td>\n",
              "      <td>The technology-laced Nasdaq Composite Index &lt;....</td>\n",
              "      <td>2.400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2229</th>\n",
              "      <td>Action is needed quickly, which is why we deci...</td>\n",
              "      <td>It is urgent and that is why we have decided t...</td>\n",
              "      <td>5.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2230</th>\n",
              "      <td>One could indeed wish for more and for improve...</td>\n",
              "      <td>We can actually want more and better, but I th...</td>\n",
              "      <td>4.800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2231</th>\n",
              "      <td>(Parliament accepted the oral amendment)</td>\n",
              "      <td>(Parliament accepted the oral amendment)</td>\n",
              "      <td>5.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2232</th>\n",
              "      <td>- My party has serious reservations about Comm...</td>\n",
              "      <td>My party serious reservations about the regula...</td>\n",
              "      <td>4.800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2233</th>\n",
              "      <td>He saw a red rose.</td>\n",
              "      <td>He drove a rose colored car.</td>\n",
              "      <td>0.800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2234 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     s1  ...     gs\n",
              "0     But other sources close to the sale said Viven...  ...  4.000\n",
              "1     Micron has declared its first quarterly profit...  ...  3.750\n",
              "2     The fines are part of failed Republican effort...  ...  2.800\n",
              "3     The American Anglican Council, which represent...  ...  3.400\n",
              "4     The tech-loaded Nasdaq composite rose 20.96 po...  ...  2.400\n",
              "...                                                 ...  ...    ...\n",
              "2229  Action is needed quickly, which is why we deci...  ...  5.000\n",
              "2230  One could indeed wish for more and for improve...  ...  4.800\n",
              "2231           (Parliament accepted the oral amendment)  ...  5.000\n",
              "2232  - My party has serious reservations about Comm...  ...  4.800\n",
              "2233                                 He saw a red rose.  ...  0.800\n",
              "\n",
              "[2234 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0yI6VaE4V7r"
      },
      "source": [
        "Importing the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "OtOwYOoiV9zu",
        "outputId": "f07586c2-972a-4afb-a505-2e7f6c7ccc04"
      },
      "source": [
        "files_test = glob.glob('/content/drive/MyDrive/IHLT/data_STS/test-gold//STS.input.*')\n",
        "gs_test = glob.glob('/content/drive/MyDrive/IHLT/data_STS/test-gold/STS.gs.*')\n",
        "df_test = create_df(files_test,gs_test)\n",
        "df_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>s1</th>\n",
              "      <th>s2</th>\n",
              "      <th>gs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The problem likely will mean corrective change...</td>\n",
              "      <td>He said the problem needs to be corrected befo...</td>\n",
              "      <td>4.400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The technology-laced Nasdaq Composite Index .I...</td>\n",
              "      <td>The broad Standard &amp; Poor's 500 Index .SPX inc...</td>\n",
              "      <td>0.800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"It's a huge black eye,\" said publisher Arthur...</td>\n",
              "      <td>\"It's a huge black eye,\" Arthur Sulzberger, th...</td>\n",
              "      <td>3.600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SEC Chairman William Donaldson said there is a...</td>\n",
              "      <td>\"I think there's a building confidence that th...</td>\n",
              "      <td>3.400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Vivendi shares closed 1.9 percent at 15.80 eur...</td>\n",
              "      <td>In New York, Vivendi shares were 1.4 percent d...</td>\n",
              "      <td>1.400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3103</th>\n",
              "      <td>A defeat on Alstom would have profound consequ...</td>\n",
              "      <td>Losing on the issue of Alstom would have serio...</td>\n",
              "      <td>4.750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3104</th>\n",
              "      <td>Tocqueville believed that there are no effecti...</td>\n",
              "      <td>Tocqueville thought that on the long run, noth...</td>\n",
              "      <td>4.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3105</th>\n",
              "      <td>Will it give us the right to divorce the husba...</td>\n",
              "      <td>A couple who have left?</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3106</th>\n",
              "      <td>But US stock prices fell only 5.2% between May...</td>\n",
              "      <td>However, the Americans have accused that lower...</td>\n",
              "      <td>3.250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3107</th>\n",
              "      <td>He did, but the initiative did not get very far.</td>\n",
              "      <td>What it did without the initiative not going v...</td>\n",
              "      <td>3.500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3108 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     s1  ...     gs\n",
              "0     The problem likely will mean corrective change...  ...  4.400\n",
              "1     The technology-laced Nasdaq Composite Index .I...  ...  0.800\n",
              "2     \"It's a huge black eye,\" said publisher Arthur...  ...  3.600\n",
              "3     SEC Chairman William Donaldson said there is a...  ...  3.400\n",
              "4     Vivendi shares closed 1.9 percent at 15.80 eur...  ...  1.400\n",
              "...                                                 ...  ...    ...\n",
              "3103  A defeat on Alstom would have profound consequ...  ...  4.750\n",
              "3104  Tocqueville believed that there are no effecti...  ...  4.500\n",
              "3105  Will it give us the right to divorce the husba...  ...  1.000\n",
              "3106  But US stock prices fell only 5.2% between May...  ...  3.250\n",
              "3107   He did, but the initiative did not get very far.  ...  3.500\n",
              "\n",
              "[3108 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG1SE7luBUYb"
      },
      "source": [
        "Getting the features from the train and test datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38IJRvVEpJhC"
      },
      "source": [
        "features_jaccard_train = get_jaccard_features(df_train)\n",
        "gs_train = np.array([float(g) for g in df_train['gs']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFwo8yV502Ct"
      },
      "source": [
        "features_jaccard_test = get_jaccard_features(df_test)\n",
        "gs_test = np.array([float(g) for g in df_test['gs']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh0hspG7Bbam"
      },
      "source": [
        "Scaling the feature lists:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4Qao64tCMdt"
      },
      "source": [
        "# scale the data frame\n",
        "scaler = MinMaxScaler()\n",
        "# for training\n",
        "scaler.fit(features_jaccard_train)\n",
        "features_jaccard_train_scaled = scaler.transform(features_jaccard_train)\n",
        "# for testing\n",
        "features_jaccard_test_scaled = scaler.transform(features_jaccard_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXJpu-4xB2DM"
      },
      "source": [
        "Setting up an SVR model and feeding the feature list from the training set as well as the golden standard from the training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQpyhnCHDeSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "676f70ff-2e5f-4560-9e7a-c54a9f5416de"
      },
      "source": [
        "kernel = 'rbf'\n",
        "gamma = 4\n",
        "C = 1\n",
        "epsilon = 0.5\n",
        "tol = 1\n",
        "\n",
        "svr = SVR(gamma = gamma, C = C, epsilon = epsilon, tol = tol)\n",
        "svr.fit(features_jaccard_train_scaled, gs_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVR(C=1, epsilon=0.5, gamma=4, tol=1)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9ZXFRxSCHvw"
      },
      "source": [
        "Testing the regression of the model on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQc_S0w_EJvX"
      },
      "source": [
        "test_prediction = svr.predict(features_jaccard_test_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbVYfGMHCOWe"
      },
      "source": [
        "Computing the pearson correlation of the predicted values from the testing set and the gold standard:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7W0wp5aEg8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c8ecc8-ef10-446e-86da-d0eb2a2ba1b4"
      },
      "source": [
        "correlation = pearsonr(test_prediction, gs_test)[0]\n",
        "print(\"Pearson correlation:\", correlation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson correlation: 0.6917634011205702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Regression:"
      ],
      "metadata": {
        "id": "-H_Nir0c9s4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the right parameters for the model:"
      ],
      "metadata": {
        "id": "yXMM8W52mkUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameter_space = {\n",
        "    'hidden_layer_sizes': [(100, 50), (200, 100), (50, 100), (40, 150)],\n",
        "    'activation': ['relu'],\n",
        "    'solver': ['sgd', 'adam','lbfgs'],\n",
        "    'alpha': [0.0001, 0.001, 0.1,0.05, 0.5],\n",
        "    'learning_rate': ['constant','adaptive','invscaling'],\n",
        "}\n",
        "clf = GridSearchCV(regr, parameter_space, n_jobs=-1, cv=5)\n",
        "clf.fit(features_jaccard_train_scaled, gs_train)\n",
        "print('Best parameters found:\\n', clf.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-5eWFpYnocn",
        "outputId": "c90b0fed-394d-4f3a-aad2-aed8060f30e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:\n",
            " {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (100, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regr = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', \n",
        "                     alpha=0.001, learning_rate='constant').fit(features_jaccard_train_scaled, gs_train)\n",
        "mlp_predictions = regr.predict(features_jaccard_test_scaled)\n",
        "correlation = pearsonr(mlp_predictions, gs_test)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__bjo4cZ9uhi",
        "outputId": "5407335e-e294-4308-e9b0-f0825932033a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2opJQpsI9xAo"
      },
      "source": [
        "Computing the pearson correlation of the predicted values from the testing set and the gold standard:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530dd7ed-1526-4413-ffaf-b76609549b32",
        "id": "8d9r9DjoKGNy"
      },
      "source": [
        "correlation = pearsonr(mlp_predictions, gs_test)[0]\n",
        "print(\"Pearson correlation:\", correlation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson correlation: 0.7117823424160945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "Comparing the similarity of sentences is not a simple task, and requires a lot of methods to perform it. There is a lot of algorithms that extract features from the sentences. We used features such as:\n",
        "- tokenizing (with and without stopwords)\n",
        "- lemmatizing (with and without stopwords)\n",
        "- getting part-of-speech tags (simple and complex)\n",
        "- name entity chunking\n",
        "- Lesk's algorithm\n",
        "- synsets similarity\n",
        "- n-grams similarity\n",
        "- n-grams frequency\n",
        "\n",
        "\n",
        "We applied jaccard distances to find the similarities between most featurized sentences. We found out that the cosine similarity only works good with comparing n-grams, because it takes into account the duplicates of elements. To combine all of the features we fed them to regression models such as Support Vector Regression and Multi Layer Perceptron Regression. We found out that the number of features is not the most important. Rather the quality of features has more influence on the overall quality of the model. We obtained 0.717 pearson correlation with MLP Regression and 0.691 with SVR. The results are simmilar, but the MLP performed better so we kept it. We found the parameters by performing GridSearchCV. We tested a lot of combinations of features and kept the most useful and deleted the ones that were creating noise:\n",
        "- n-grams similarity (lots of not meaningful data)\n",
        "- name entity chunking (only the entities were returned, and because of that, the data was insufficient to perform comparisons)\n",
        "- complex part-of-speech tagging (too complex, and we already performed the simple version)\n",
        "\n",
        "\n",
        "We also found out that the regression methods are the best for combining these features, because they find correlation between all of them and can output a final result. There is still area to improve, for example searching bigger parameter spaces in the MLP model. For that however, we would need more computing power which we did not have."
      ],
      "metadata": {
        "id": "7c4v9vwcEyul"
      }
    }
  ]
}